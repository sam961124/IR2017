{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Input, LSTM, RepeatVector, Embedding, TimeDistributed, Dense\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import h5py\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_path = '../HW3/IRTM/'\n",
    "num_doc = 1095\n",
    "MAX_NUM_WORDS = 7000\n",
    "MAX_SENTENSE_LENGTH = 66\n",
    "MAX_DOC_LENGTH = 28\n",
    "VALID_SIZE = 200\n",
    "WORD_EMBEDDING_DIM = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_doc = [[] for i in range(num_doc)]\n",
    "for i in range(num_doc):\n",
    "    doc_file = open(doc_path + str(i+1) + '.txt', 'r')\n",
    "    article = doc_file.read()\n",
    "    all_doc[i] = article\n",
    "all_doc = np.array(all_doc)\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(all_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tokenizer.texts_to_sequences(all_doc)\n",
    "t_doc = tokenizer.sequences_to_matrix(temp, mode='count')\n",
    "t_doc /= MAX_NUM_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_doc = []\n",
    "# for i in range(num_doc):\n",
    "#     s_temp = []\n",
    "#     sentenses = nltk.sent_tokenize(all_doc[i])\n",
    "#     temp = tokenizer.texts_to_sequences(sentenses)\n",
    "#     s_temp = pad_sequences(temp, maxlen=MAX_SENTENSE_LENGTH)\n",
    "#     training_doc.append(s_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# t_doc = pad_sequences(training_doc, maxlen=MAX_DOC_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "indices = np.random.permutation(len(t_doc))\n",
    "x_train = t_doc[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_dim = 15\n",
    "dnn_encode = [250, 125, 13]\n",
    "dnn_decode = [250, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(MAX_NUM_WORDS,))\n",
    "encoded = Dense(500, activation='relu')(inputs)\n",
    "encoded = BatchNormalization()(encoded)\n",
    "encoded = Dropout(0.3)(encoded)\n",
    "for units in dnn_encode:\n",
    "    encoded = Dense(units, activation='relu')(encoded)\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    encoded = Dropout(0.3)(encoded)\n",
    "encoder = Model(inputs, encoded)\n",
    "\n",
    "decoded = Dense(125, activation='relu')(encoded)\n",
    "decoded = BatchNormalization()(decoded)\n",
    "decoded = Dropout(0.3)(decoded)\n",
    "for units in dnn_decode:\n",
    "    decoded = Dense(units, activation='relu')(decoded)\n",
    "    decoded = BatchNormalization()(decoded)\n",
    "    decoded = Dropout(0.3)(decoded)\n",
    "decoded = Dense(MAX_NUM_WORDS, activation='softmax')(decoded)\n",
    "autoencoder = Model(inputs, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = Input(shape=(MAX_DOC_LENGTH, MAX_SENTENSE_LENGTH))\n",
    "# embeddings = Embedding(MAX_NUM_WORDS, WORD_EMBEDDING_DIM)(inputs)\n",
    "# encoded = TimeDistributed(LSTM(latent_dim))(embeddings)\n",
    "# encoded = LSTM(latent_dim)(encoded)\n",
    "# encoder = Model(inputs, encoded)\n",
    "# decoded = RepeatVector(MAX_DOC_LENGTH)(encoded)\n",
    "# decoded = LSTM(MAX_SENTENSE_LENGTH, return_sequences=True)(decoded)\n",
    "# # decoded = Dense(vocab_size, activation='softmax')(decoded)\n",
    "# sequence_autoencoder = Model(inputs, decoded)\n",
    "# sequence_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1095/1095 [==============================] - 11s - loss: 0.5383 - acc: 0.3790    \n",
      "Epoch 2/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4910 - acc: 0.6594     \n",
      "Epoch 3/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4751 - acc: 0.8594     \n",
      "Epoch 4/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4714 - acc: 0.8986     \n",
      "Epoch 5/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4701 - acc: 0.9041     \n",
      "Epoch 6/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4673 - acc: 0.8986     \n",
      "Epoch 7/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4656 - acc: 0.9078     \n",
      "Epoch 8/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4640 - acc: 0.9087     \n",
      "Epoch 9/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4641 - acc: 0.9032     \n",
      "Epoch 10/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4625 - acc: 0.9105     \n",
      "Epoch 11/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4613 - acc: 0.9096     \n",
      "Epoch 12/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4597 - acc: 0.9032     \n",
      "Epoch 13/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4591 - acc: 0.9087     \n",
      "Epoch 14/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4580 - acc: 0.9114     \n",
      "Epoch 15/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4573 - acc: 0.9105     \n",
      "Epoch 16/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4569 - acc: 0.9105     \n",
      "Epoch 17/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4563 - acc: 0.9151     \n",
      "Epoch 18/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4555 - acc: 0.9160     \n",
      "Epoch 19/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4546 - acc: 0.9151     \n",
      "Epoch 20/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4542 - acc: 0.9142     \n",
      "Epoch 21/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4539 - acc: 0.9151     \n",
      "Epoch 22/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4538 - acc: 0.9123     \n",
      "Epoch 23/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4534 - acc: 0.9151     \n",
      "Epoch 24/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4530 - acc: 0.9178     \n",
      "Epoch 25/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4522 - acc: 0.9151     \n",
      "Epoch 26/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4517 - acc: 0.9142     \n",
      "Epoch 27/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4505 - acc: 0.9151     \n",
      "Epoch 28/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4504 - acc: 0.9151     \n",
      "Epoch 29/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4499 - acc: 0.9142     \n",
      "Epoch 30/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4493 - acc: 0.9160     \n",
      "Epoch 31/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4489 - acc: 0.9187     \n",
      "Epoch 32/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4495 - acc: 0.9151     \n",
      "Epoch 33/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4486 - acc: 0.9151     \n",
      "Epoch 34/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4489 - acc: 0.9178     \n",
      "Epoch 35/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4486 - acc: 0.9178     \n",
      "Epoch 36/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4481 - acc: 0.9178     \n",
      "Epoch 37/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4485 - acc: 0.9169     \n",
      "Epoch 38/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4480 - acc: 0.9160     \n",
      "Epoch 39/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4480 - acc: 0.9160     \n",
      "Epoch 40/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4472 - acc: 0.9178     \n",
      "Epoch 41/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4464 - acc: 0.9178     \n",
      "Epoch 42/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4470 - acc: 0.9187     \n",
      "Epoch 43/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4462 - acc: 0.9178     \n",
      "Epoch 44/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4461 - acc: 0.9187     \n",
      "Epoch 45/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4462 - acc: 0.9178     \n",
      "Epoch 46/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4458 - acc: 0.9187     \n",
      "Epoch 47/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4459 - acc: 0.9187     \n",
      "Epoch 48/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4462 - acc: 0.9187     \n",
      "Epoch 49/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4461 - acc: 0.9178     \n",
      "Epoch 50/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4455 - acc: 0.9178     \n",
      "Epoch 51/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4449 - acc: 0.9178     \n",
      "Epoch 52/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4449 - acc: 0.9187     \n",
      "Epoch 53/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4458 - acc: 0.9169     \n",
      "Epoch 54/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4450 - acc: 0.9187     \n",
      "Epoch 55/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4451 - acc: 0.9187     \n",
      "Epoch 56/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4448 - acc: 0.9187     \n",
      "Epoch 57/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4441 - acc: 0.9178     \n",
      "Epoch 58/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4439 - acc: 0.9178     \n",
      "Epoch 59/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4444 - acc: 0.9178     \n",
      "Epoch 60/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4438 - acc: 0.9187     \n",
      "Epoch 61/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4438 - acc: 0.9187     \n",
      "Epoch 62/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4443 - acc: 0.9178     \n",
      "Epoch 63/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4440 - acc: 0.9187     \n",
      "Epoch 64/100\n",
      "1095/1095 [==============================] - 4s - loss: 0.4440 - acc: 0.9187     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bb218d6d68>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = []\n",
    "callbacks.append(ModelCheckpoint('model.h5', monitor='acc', \n",
    "                                 save_best_only=True, period=1))\n",
    "callbacks.append(EarlyStopping(monitor='loss', patience=3))\n",
    "autoencoder.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "autoencoder.fit(x_train, x_train, epochs=100, batch_size=5, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        (None, 7000)              0         \n",
      "_________________________________________________________________\n",
      "dense_160 (Dense)            (None, 500)               3500500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_138 (Bat (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dropout_138 (Dropout)        (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_161 (Dense)            (None, 250)               125250    \n",
      "_________________________________________________________________\n",
      "batch_normalization_139 (Bat (None, 250)               1000      \n",
      "_________________________________________________________________\n",
      "dropout_139 (Dropout)        (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_162 (Dense)            (None, 125)               31375     \n",
      "_________________________________________________________________\n",
      "batch_normalization_140 (Bat (None, 125)               500       \n",
      "_________________________________________________________________\n",
      "dropout_140 (Dropout)        (None, 125)               0         \n",
      "_________________________________________________________________\n",
      "dense_163 (Dense)            (None, 13)                1638      \n",
      "_________________________________________________________________\n",
      "batch_normalization_141 (Bat (None, 13)                52        \n",
      "_________________________________________________________________\n",
      "dropout_141 (Dropout)        (None, 13)                0         \n",
      "_________________________________________________________________\n",
      "dense_164 (Dense)            (None, 125)               1750      \n",
      "_________________________________________________________________\n",
      "batch_normalization_142 (Bat (None, 125)               500       \n",
      "_________________________________________________________________\n",
      "dropout_142 (Dropout)        (None, 125)               0         \n",
      "_________________________________________________________________\n",
      "dense_165 (Dense)            (None, 250)               31500     \n",
      "_________________________________________________________________\n",
      "batch_normalization_143 (Bat (None, 250)               1000      \n",
      "_________________________________________________________________\n",
      "dropout_143 (Dropout)        (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_166 (Dense)            (None, 500)               125500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_144 (Bat (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dropout_144 (Dropout)        (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_167 (Dense)            (None, 7000)              3507000   \n",
      "=================================================================\n",
      "Total params: 7,331,565\n",
      "Trainable params: 7,328,039\n",
      "Non-trainable params: 3,526\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "aut = load_model('model.h5')\n",
    "aut.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = K.function([aut.layers[0].input, K.learning_phase()], [aut.layers[10].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (f([t_doc[323].reshape(1, 7000), 0])[0])\n",
    "b = (f([t_doc[10].reshape(1, 7000), 0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.94114035]], dtype=float32)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_class = []\n",
    "training_file = pd.read_table('../HW3/training.txt', header=None)\n",
    "for i in range(13):\n",
    "    temp = training_file[0][i].split(' ')\n",
    "    doc_class.append(temp[1:-1])\n",
    "doc_class = np.array(doc_class, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  11,   19,   29,  113,  115,  169,  278,  301,  316,  317,  321,\n",
       "         324,  325,  338,  341],\n",
       "       [   1,    2,    3,    4,    5,    6,    7,    8,    9,   10,   12,\n",
       "          13,   14,   15,   16],\n",
       "       [ 813,  817,  818,  819,  820,  821,  822,  824,  825,  826,  828,\n",
       "         829,  830,  832,  833],\n",
       "       [ 635,  680,  683,  702,  704,  705,  706,  708,  709,  719,  720,\n",
       "         722,  723,  724,  726],\n",
       "       [ 646,  751,  781,  794,  798,  799,  801,  812,  815,  823,  831,\n",
       "         839,  840,  841,  842],\n",
       "       [ 995,  998,  999, 1003, 1005, 1006, 1007, 1009, 1011, 1012, 1013,\n",
       "        1014, 1015, 1016, 1019],\n",
       "       [ 700,  730,  731,  732,  733,  735,  740,  744,  752,  754,  755,\n",
       "         756,  757,  759,  760],\n",
       "       [ 262,  296,  304,  308,  337,  397,  401,  443,  445,  450,  466,\n",
       "         480,  513,  533,  534],\n",
       "       [ 130,  131,  132,  133,  134,  135,  136,  137,  138,  139,  140,\n",
       "         141,  142,  143,  145],\n",
       "       [  31,   44,   70,   83,   86,   92,  100,  102,  305,  309,  315,\n",
       "         320,  326,  327,  328],\n",
       "       [ 240,  241,  243,  244,  245,  248,  250,  254,  255,  256,  258,\n",
       "         260,  275,  279,  295],\n",
       "       [ 535,  542,  571,  573,  574,  575,  576,  578,  581,  582,  583,\n",
       "         584,  585,  586,  588],\n",
       "       [ 485,  520,  523,  526,  527,  529,  530,  531,  532,  536,  537,\n",
       "         538,  539,  540,  541]])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11 ,12, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in range(len(t_doc)):\n",
    "    temp = f([t_doc[i].reshape(1,7000), 0])[0]\n",
    "    X.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing t-SNE embedding\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# t-SNE embedding of the digits dataset\n",
    "print(\"Computing t-SNE embedding\")           \n",
    "tsne = TSNE(n_components=2, init='pca', random_state=0)\n",
    "X_tsne = tsne.fit_transform(X.reshape(1095, 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJCUU8EVEUuL/AwJKl2DivLNmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bb2e89b240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = ['#FFB7DD', '#FFBB66', '#FFFF33', '#BBFF00', '#CC0000', '#33CCFF', '#9999FF',\n",
    "'#00AA55', '#E8CCFF', '#770077', '#FF7744', '#DDDDDD', '#444444']\n",
    "for i, c, label in zip(doc_class, colors, labels):\n",
    "    plt.scatter(X_tsne[i-1, 0], X_tsne[i-1, 1], c=c, label=label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
