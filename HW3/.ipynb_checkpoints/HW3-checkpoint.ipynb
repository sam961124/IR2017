{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.corpus as corpus\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# get stopwords\n",
    "stopWords = set(corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 13\n",
    "doc_path = 'IRTM/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_class = []\n",
    "training_file = pd.read_table('training.txt', header=None)\n",
    "for i in range(classes):\n",
    "    temp = training_file[0][i].split(' ')\n",
    "    doc_class.append(temp[1:-1])\n",
    "doc_class = np.array(doc_class)\n",
    "training_doc_flat = doc_class.flatten()\n",
    "\n",
    "# split training & valid set\n",
    "valid_labels = []\n",
    "valid_size = 0\n",
    "train_doc = doc_class[:,:15-valid_size]\n",
    "valid_doc = doc_class[:,15-valid_size:]\n",
    "for c in range(classes):\n",
    "    for i in range(valid_size):\n",
    "        valid_labels.append(c+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractVocabulary(doc):\n",
    "    temp_dict = {}\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'[a-zA-Z]+')\n",
    "    count = 1\n",
    "    for i in range(len(doc)):\n",
    "        doc_file = open(doc_path + doc[i] + '.txt', 'r')\n",
    "        article = doc_file.read()\n",
    "        words = tokenizer.tokenize(article)\n",
    "        for w in words:\n",
    "            temp = str(ps.stem(w.lower()))\n",
    "            if temp not in stopWords:\n",
    "                if temp not in temp_dict:\n",
    "                    temp_dict[temp] = count\n",
    "                    count += 1\n",
    "    return(temp_dict)\n",
    "\n",
    "def BuildFeaturesLabels(doc_class, vocab):\n",
    "    labels = np.zeros(195)\n",
    "    features = np.zeros((195, len(vocab)+1))\n",
    "    count = 0\n",
    "    for c in range(classes):\n",
    "        for d in doc_class[c]:\n",
    "            labels[count] = c+1\n",
    "            doc_vocab = ExtractVocabulary([d])\n",
    "            terms = []\n",
    "            for v in doc_vocab:\n",
    "                features[count][vocab[v]] = 1\n",
    "            count += 1\n",
    "    return features, labels\n",
    "    \n",
    "def ComputeChi(features, labels):\n",
    "    N = len(labels)\n",
    "    term_chi = []\n",
    "    for t in range(features.shape[1]):\n",
    "        temp = 0\n",
    "        for c in range(classes):\n",
    "            start_idx = c*15\n",
    "            end_idx = (c+1)*15\n",
    "            presents = features[:,t].sum()\n",
    "            absents = N - presents\n",
    "            on_topic_present = features[start_idx:end_idx,t].sum()\n",
    "            off_topic_present = presents - on_topic_present\n",
    "            on_topic_absent = 15 - on_topic_present\n",
    "            off_topic_absent = absents - on_topic_absent\n",
    "            E = N * (on_topic_present + off_topic_present)/N * (on_topic_present + \n",
    "                                                                on_topic_absent)/N\n",
    "            temp += (N - E)**2/E\n",
    "        term_chi.append(temp/classes)\n",
    "    return term_chi\n",
    "            \n",
    "    \n",
    "def SelectFeatures(all_vocab, features, labels, num):\n",
    "    chi_list = ComputeChi(features, labels)\n",
    "    index = np.argsort(chi_list)\n",
    "    selected_vocab = {k: v for k, v in all_vocab.items() if v in index[:500]}\n",
    "    return(index[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConcatTextInClass(doc_class, c, vocab):\n",
    "    text = []\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'[a-zA-Z]+')\n",
    "    for d in doc_class[c]:\n",
    "        doc_file = open(doc_path + d + '.txt', 'r')\n",
    "        article = doc_file.read()\n",
    "        words = tokenizer.tokenize(article)\n",
    "        for w in words:\n",
    "            temp = str(ps.stem(w.lower()))\n",
    "            if temp not in stopWords and temp in vocab:\n",
    "                text.append(temp)\n",
    "    return text\n",
    "\n",
    "def TrainMultinomialNB(train_doc, vocab):\n",
    "    N = 195 - classes * valid_size\n",
    "    prior = np.zeros(classes)\n",
    "    condprob = np.zeros((len(vocab), classes))\n",
    "    class_count = np.unique(labels, return_counts=True)\n",
    "    for c in range(classes):\n",
    "        T = np.zeros(len(vocab))\n",
    "        Nc = 15 - valid_size\n",
    "        prior[c] = Nc / N\n",
    "        text_c = ConcatTextInClass(train_doc, c, vocab)\n",
    "        for t in range(len(vocab)):\n",
    "            count = 0\n",
    "            for v in text_c:\n",
    "                if v == vocab[t]:\n",
    "                    count += 1\n",
    "            T[t] = count\n",
    "        for t in range(len(vocab)):\n",
    "            condprob[t][c] = (T[t] + 1) / (len(text_c) + len(vocab))\n",
    "    return prior, condprob\n",
    "\n",
    "def ExtractTokensFromDocs(vocab, d):\n",
    "    text = []\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'[a-zA-Z]+')\n",
    "    doc_file = open(doc_path + d + '.txt', 'r')\n",
    "    article = doc_file.read()\n",
    "    words = tokenizer.tokenize(article)\n",
    "    for w in words:\n",
    "        temp = str(ps.stem(w.lower()))\n",
    "        if temp not in stopWords and temp in vocab:\n",
    "            text.append(vocab.index(temp))\n",
    "    return text\n",
    "\n",
    "def ApplyMultinomialNB(vocab, prior, condprob, d):\n",
    "    W = ExtractTokensFromDocs(vocab, d)\n",
    "    score = np.zeros(classes)\n",
    "    for c in range(classes):\n",
    "        score[c] = np.log(prior[c])\n",
    "        for t in W:\n",
    "            score[c] += np.log(condprob[t][c])\n",
    "    return np.argmax(score) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sean\\anaconda3\\envs\\ir\\lib\\site-packages\\ipykernel_launcher.py:48: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# feature selection\n",
    "all_vocab = ExtractVocabulary(training_doc_flat)\n",
    "features, labels = BuildFeaturesLabels(doc_class, all_vocab)\n",
    "selected_index = SelectFeatures(all_vocab, features, labels, 500)\n",
    "selected_vocab = [k for k, v in all_vocab.items() if v in selected_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train NB\n",
    "prior, condprob = TrainMultinomialNB(train_doc, selected_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output\n",
    "all_index = np.arange(1,1096,1)\n",
    "train_index = np.array(train_doc.flatten(), dtype=int)\n",
    "output_index = list(set(all_index).difference(set(train_index)))\n",
    "text_file = open(\"output.txt\", \"w\")\n",
    "for d in output_index:\n",
    "    res = ApplyMultinomialNB(selected_vocab, prior, condprob, str(d))\n",
    "    text_file.write(str(d) + ' ' + str(res) + '\\n')\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation accuracy\n",
    "# accuracy = 0\n",
    "# c = 0\n",
    "# for d in valid_doc.flatten():\n",
    "#     res = ApplyMultinomialNB(selected_vocab, prior, condprob, d)\n",
    "#     if res == valid_labels[c]:\n",
    "#         accuracy += 1\n",
    "#     c += 1\n",
    "# accuracy /= classes * valid_size\n",
    "# print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
