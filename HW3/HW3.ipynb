{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.corpus as corpus\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# get stopwords\n",
    "stopWords = set(corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = 13\n",
    "doc_path = 'IRTM/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_class = []\n",
    "training_file = pd.read_table('training.txt', header=None)\n",
    "for i in range(classes):\n",
    "    temp = training_file[0][i].split(' ')\n",
    "    doc_class.append(temp[1:-1])\n",
    "doc_class = np.array(doc_class)\n",
    "training_doc_flat = doc_class.flatten()\n",
    "\n",
    "# split training & valid set\n",
    "valid_labels = []\n",
    "valid_size = 0\n",
    "train_doc = doc_class[:,:15-valid_size]\n",
    "valid_doc = doc_class[:,15-valid_size:]\n",
    "for c in range(classes):\n",
    "    for i in range(valid_size):\n",
    "        valid_labels.append(c+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ExtractVocabulary(doc):\n",
    "    temp_dict = {}\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'[a-zA-Z]+')\n",
    "    count = 1\n",
    "    for i in range(len(doc)):\n",
    "        doc_file = open(doc_path + doc[i] + '.txt', 'r')\n",
    "        article = doc_file.read()\n",
    "        words = tokenizer.tokenize(article)\n",
    "        for w in words:\n",
    "            temp = str(ps.stem(w.lower()))\n",
    "            if temp not in stopWords:\n",
    "                if temp not in temp_dict:\n",
    "                    temp_dict[temp] = count\n",
    "                    count += 1\n",
    "    return(temp_dict)\n",
    "\n",
    "def BuildFeaturesLabels(doc_class, vocab):\n",
    "    labels = np.zeros(195)\n",
    "    features = np.zeros((195, len(vocab)+1))\n",
    "    count = 0\n",
    "    for c in range(classes):\n",
    "        for d in doc_class[c]:\n",
    "            labels[count] = c+1\n",
    "            doc_vocab = ExtractVocabulary([d])\n",
    "            terms = []\n",
    "            for v in doc_vocab:\n",
    "                features[count][vocab[v]] = 1\n",
    "            count += 1\n",
    "    return features, labels\n",
    "    \n",
    "def ComputeChi(features, labels):\n",
    "    N = len(labels)\n",
    "    term_chi = []\n",
    "    for t in range(features.shape[1]):\n",
    "        temp = 0\n",
    "        for c in range(classes):\n",
    "            start_idx = c*15\n",
    "            end_idx = (c+1)*15\n",
    "            presents = features[:,t].sum()\n",
    "            absents = N - presents\n",
    "            on_topic_present = features[start_idx:end_idx,t].sum()\n",
    "            off_topic_present = presents - on_topic_present\n",
    "            on_topic_absent = 15 - on_topic_present\n",
    "            off_topic_absent = absents - on_topic_absent\n",
    "            E = N * (on_topic_present + off_topic_present)/N * (on_topic_present + \n",
    "                                                                on_topic_absent)/N\n",
    "            temp += (on_topic_present - E)**2/E\n",
    "        term_chi.append(temp/classes)\n",
    "    return term_chi\n",
    "            \n",
    "    \n",
    "def SelectFeatures(all_vocab, features, labels, num):\n",
    "    chi_list = ComputeChi(features, labels)\n",
    "    index = np.argsort(chi_list)\n",
    "    selected_vocab = {k: v for k, v in all_vocab.items() if v in index[:num]}\n",
    "    return(index[-num-1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ConcatTextInClass(doc_class, c, vocab):\n",
    "    text = []\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'[a-zA-Z]+')\n",
    "    for d in doc_class[c]:\n",
    "        doc_file = open(doc_path + d + '.txt', 'r')\n",
    "        article = doc_file.read()\n",
    "        words = tokenizer.tokenize(article)\n",
    "        for w in words:\n",
    "            temp = str(ps.stem(w.lower()))\n",
    "            if temp not in stopWords and temp in vocab:\n",
    "                text.append(temp)\n",
    "    return text\n",
    "\n",
    "def TrainMultinomialNB(train_doc, vocab):\n",
    "    N = 195 - classes * valid_size\n",
    "    prior = np.zeros(classes)\n",
    "    condprob = np.zeros((len(vocab), classes))\n",
    "    class_count = np.unique(labels, return_counts=True)\n",
    "    for c in range(classes):\n",
    "        T = np.zeros(len(vocab))\n",
    "        Nc = 15 - valid_size\n",
    "        prior[c] = Nc / N\n",
    "        text_c = ConcatTextInClass(train_doc, c, vocab)\n",
    "        for t in range(len(vocab)):\n",
    "            count = 0\n",
    "            for v in text_c:\n",
    "                if v == vocab[t]:\n",
    "                    count += 1\n",
    "            T[t] = count\n",
    "        for t in range(len(vocab)):\n",
    "            condprob[t][c] = (T[t] + 1) / (len(text_c) + len(vocab))\n",
    "    return prior, condprob\n",
    "\n",
    "def ExtractTokensFromDocs(vocab, d):\n",
    "    text = []\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'[a-zA-Z]+')\n",
    "    doc_file = open(doc_path + d + '.txt', 'r')\n",
    "    article = doc_file.read()\n",
    "    words = tokenizer.tokenize(article)\n",
    "    for w in words:\n",
    "        temp = str(ps.stem(w.lower()))\n",
    "        if temp not in stopWords and temp in vocab:\n",
    "            text.append(vocab.index(temp))\n",
    "    return text\n",
    "\n",
    "def ApplyMultinomialNB(vocab, prior, condprob, d):\n",
    "    W = ExtractTokensFromDocs(vocab, d)\n",
    "    score = np.zeros(classes)\n",
    "    for c in range(classes):\n",
    "        score[c] = np.log(prior[c])\n",
    "        for t in W:\n",
    "            score[c] += np.log(condprob[t][c])\n",
    "    return np.argmax(score) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sean\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# feature selection\n",
    "all_vocab = ExtractVocabulary(training_doc_flat)\n",
    "features, labels = BuildFeaturesLabels(doc_class, all_vocab)\n",
    "selected_index = SelectFeatures(all_vocab, features, labels, 500)\n",
    "selected_vocab = [k for k, v in all_vocab.items() if v in selected_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train NB\n",
    "prior, condprob = TrainMultinomialNB(train_doc, selected_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output\n",
    "all_index = np.arange(1,1096,1)\n",
    "train_index = np.array(train_doc.flatten(), dtype=int)\n",
    "output_index = list(set(all_index).difference(set(train_index)))\n",
    "text_file = open(\"r06725015.txt\", \"w\")\n",
    "for d in output_index:\n",
    "    res = ApplyMultinomialNB(selected_vocab, prior, condprob, str(d))\n",
    "    text_file.write(str(d) + '\\t' + str(res) + '\\n')\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d7db046e76ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0maccuracy\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mclasses\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mvalid_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# validation accuracy\n",
    "accuracy = 0\n",
    "c = 0\n",
    "for d in valid_doc.flatten():\n",
    "    res = ApplyMultinomialNB(selected_vocab, prior, condprob, d)\n",
    "    if res == valid_labels[c]:\n",
    "        accuracy += 1\n",
    "    c += 1\n",
    "accuracy /= classes * valid_size\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['navi', 'japan', 'monday', 'submarin', 'rescu', 'pacif', 'ship', 'china', 'sea', 'russian', 'hull', 'asia', 'war', 'russia', 'sub', 'dive', 'suit', 'nuclear', 'attack', 'uss', 'defens', 'defeat', 'militari', 'constitut', 'august', 'kursk', 'sank', 'explos', 'norwegian', 'recov', 'seamen', 'die', 'retriev', 'dalla', 'chief', 'resign', 'vice', 'presidenti', 'st', 'compart', 'destroy', 'good', 'buri', 'collis', 'deni', 'torpedo', 'barent', 'sailor', 'moscow', 'class', 'death', 'aboard', 'bodi', 'diver', 'former', 'polit', 'speed', 'store', 'second', 'boat', 'sound', 'secret', 'sunken', 'men', 'port', 'mission', 'crash', 'depart', 'rule', 'area', 'rescuer', 'least', 'damag', 'tuesday', 'night', 'weather', 'western', 'seven', 'white', 'yugoslavia', 'opposit', 'slobodan', 'milosev', 'belgrad', 'serbia', 'eve', 'strike', 'elect', 'victim', 'serb', 'coal', 'economi', 'electr', 'reserv', 'draw', 'bare', 'leader', 'civil', 'disobedi', 'offic', 'yugoslav', 'vojislav', 'kostunica', 'runoff', 'conced', 'democrat', 'candid', 'vote', 'fraud', 'court', 'mediat', 'clinton', 'polic', 'ballot', 'serbian', 'church', 'kosovo', 'wilson', 'visit', 'ceremoni', 'control', 'son', 'campaign', 'larri', 'announc', 'play', 'corrupt', 'tom', 'republ', 'absolut', 'exclud', 'debat', 'stick', 'obstacl', 'violenc', 'east', 'buy', 'central', 'health', 'build', 'econom', 'pope', 'terror', 'communist', 'accus', 'miss', 'commut', 'coast', 'weapon', 'bribe', 'bomb', 'declar', 'india', 'player', 'trial', 'america', 'aftershock', 'earthquak', 'mexico', 'magnitud', 'quak', 'el', 'salvador', 'guatemala', 'knock', 'dead', 'san', 'dig', 'collaps', 'miguel', 'santa', 'request', 'landslid', 'salvadoran', 'frantic', 'northwest', 'injur', 'dirt', 'structur', 'middl', 'cross', 'francisco', 'aid', 'sway', 'epicent', 'shake', 'la', 'colina', 'neighborhood', 'carlo', 'estim', 'toll', 'flore', 'hondura', 'arturo', 'magana', 'wander', 'jaim', 'mass', 'concret', 'shovel', 'wall', 'woman', 'ruin', 'survivor', 'ana', 'calvario', 'employe', 'worship', 'rev', 'robert', 'sosonati', 'denver', 'colo', 'spotti', 'window', 'girl', 'jalpataua', 'suchitepequez', 'temporarili', 'cellular', 'honduran', 'rubbl', 'amid', 'ridg', 'john', 'process', 'silenc', 'christma', 'antonio', 'race', 'pilot', 'parent', 'panama', 'juan', 'clemenc', 'leonard', 'peltier', 'indian', 'activist', 'convict', 'fbi', 'agent', 'dakota', 'jack', 'robberi', 'suspect', 'pine', 'sentenc', 'parol', 'pardon', 'whitewat', 'junk', 'bond', 'michael', 'milken', 'susan', 'arkansa', 'feloni', 'rare', 'nativ', 'suprem', 'loui', 'freeh', 'enforc', 'prison', 'ron', 'civilian', 'influenc', 'lawyer', 'spi', 'israel', 'act', 'texa', 'identifi', 'respons', 'mike', 'gov', 'longtim', 'isra', 'espionag', 'relat', 'espi', 'businessman', 'scandal', 'congress', 'game', 'crimin', 'tournament', 'grand', 'slam', 'semifin', 'match', 'pete', 'sampra', 'gustavo', 'kuerten', 'andr', 'agassi', 'marat', 'safin', 'australian', 'wimbledon', 'beat', 'tenni', 'yevgeni', 'kafelnikov', 'champion', 'sixth', 'titl', 'sprain', 'melbourn', 'elbow', 'ferrero', 'seed', 'martina', 'hingi', 'lindsay', 'davenport', 'venu', 'monica', 'sele', 'trade', 'ace', 'serena', 'arena', 'bother', 'cancer', 'seat', 'vodafon', 'pakistan', 'delhi', 'panic', 'gujarat', 'alert', 'terrorist', 'wreckag', 'arabian', 'nepal', 'casualti', 'contest', 'gen', 'condol', 'north', 'lawmak', 'bone', 'escap', 'madrid', 'missouri', 'trip', 'ho', 'plane', 'maximum', 'connal', 'kenedi', 'pistol', 'rifl', 'shotgun', 'ammunit', 'escape', 'harper', 'henri', 'riva', 'april', 'sport', 'oshman', 'tull', 'aubri', 'hawkin', 'robber', 'herd', 'irv', 'holdup', 'aubrey', 'fugit', 'arrest', 'roger', 'decemb', 'marco', 'account', 'senat', 'edmund', 'intellig', 'edmond', 'testimoni', 'remiss', 'congressman', 'brunei', 'peninsula', 'cole', 'aden', 'yemen', 'refuel', 'pentagon', 'notifi', 'palestinian', 'suicid', 'dock', 'alongsid', 'despic', 'cowardli', 'deter', 'utterli', 'tore', 'twin', 'africa', 'ivori', 'ouattara', 'alassan', 'draman', 'junta', 'republican', 'african', 'guei', 'ex', 'ivorian', 'coup', 'abidjan', 'rice', 'uniform', 'candidaci', 'export', 'coffe', 'cocoa', 'konan', 'bedi', 'laurent', 'gbagbo', 'exclus', 'mel', 'jean', 'carnahan', 'governor', 'chri', 'sifford', 'jefferson', 'counti', 'hilli', 'ashcroft', 'jerri', 'radar', 'sky', 'gore', 'rolla', 'lieuten', 'mo', 'agreement', 'encount', 'cessna', 'summit', 'mourner', 'peru', 'alberto', 'fujimori', 'peruvian', 'asylum', 'tokyo', 'federico', 'sala', 'radioprograma', 'ricardo', 'marquez', 'japanes', 'lima', 'ancestr', 'vladimiro', 'montesino', 'authoritarian', 'valentin', 'paniagua', 'ibero', 'portugues', 'transpar', 'leak', 'fist', 'tudela', 'launder', 'relationship', 'traffick', 'vietnam', 'chi', 'minh', 'saigon', 'vietnames', 'farewel', 'jiang', 'zemin', 'korean', 'dae', 'jung', 'yoshiro', 'mori', 'hanoi', 'unifi', 'paddi', 'repatri', 'chelsea', 'pow', 'commerc', 'societi']\n"
     ]
    }
   ],
   "source": [
    "print(selected_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
